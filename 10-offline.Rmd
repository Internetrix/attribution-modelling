```{r include = FALSE}
if(!knitr:::is_html_output())
{
  options("width"=56)
  knitr::opts_chunk$set(tidy.opts=list(width.cutoff=56, indent = 2), tidy = TRUE)
  knitr::opts_chunk$set(fig.pos = 'H')
}
```

# (PART) Offline Attribution {-}  

# About Offline Attribution

So far we have looked exclusively at digital marketing. However offline media channels also play an important part for many businesses. Generally speaking, the offline media channels include TV, broadcast, newspaper, coupons and outdoor advertising etc. Compared to online attribution, it is more difficult to measure the impact of offline marketing interventions.

When a randomised controlled experiment is not possible, an inferential method can
assist. Below we demonstrate one such method for causal inference.

Developed by Google, the `CausalImpact` R package implements methods for
causal inference using Bayesian structural time-series models [@causalimpact].

Lets see how this works.

## Scenario

*You run a business that sells widgets. Throughout the year your product demand
and website traffic goes up and down based on a variety of seasonal factors.  
You decide to run a TV commercial to promote your product coming into a busy 
time of year. This advertisement kicks off on a specified launch date and runs 
in just one of your markets (not all regions).*   

How can we measure the impact of this TV advertisement on website traffic?  

The intuitive answer is measure the change in sessions before and after the launch
date of the commercial. If we want to get more sophisticated we might try 
to compare the change in our advertising region vs. all others. 

However, a key problem here is recognising what would have happened if we 
**didn't** run our TV commercial.  If we timed our campaign to coincide with a 
busy time of year, how much of our uplift is due to the advertising versus
just organic uplift? Are we giving the advertising channel too much credit and 
inflating our ROI estimates?  

## Causal Inference Modelling    

The solution uses a three step process:  

1) Identify a *control group* which in that case could be website sessions from
another unaffected region.
2) Using historical data from our advertising region, construct a model that 
predicts what would have happened in our advertising region during the campaign 
period if no action was taken. This is called the *counterfactual*.
3) Compare this counterfactural prediction with the actual number of website
sessions to calculate the actual uplift attributable to our TV commercial.

It is important that the control group selected is not impacted by the campaign
in any way, otherwise the results may be misleading.

Lets first load the packages required:

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(CausalImpact)
library(tidyverse)
```

For this example we will simulate some time series data.

The red dashed line is the web traffic for our control region. The solid black
line is the web traffic for our region where the TV commercial is airing.

We will select our TV campaign period to be from day 70 - 100 with an uplift 
of 10 sessions per day.

```{r}
set.seed(2018)
x1 <- 100 + arima.sim(model = list(order = c(1,1,0), ar = 0.7), n = 99)
y <- 1.2 * x1 + rnorm(100)
y[70:100] <- y[70:100] + 10
data <- cbind(y, x1)

matplot(data, type = "l")

pre.period <- c(1, 70)
post.period <- c(71, 100)
```

We can now fit the causal inference model using the `CausalImpact()` function.  

```{r}
impact <- CausalImpact(data, pre.period, post.period)
```

```{r echo=FALSE}
impact
```

We can also view the results graphically. 

The 'original' facet shows the actual website visits in the black solid line and predicted values without marketing intervention in the blue dashed line. The period of intervention is shown as vertical dashed lines. The confidence intervals is shaded in blue.

The second, 'pointwise' graph basically shows the difference between the actual values and the predicted values.

The third, 'cumulative' graph shows the summed effect of the marketing intervention after accumulating the differences caused by the marketing activity since the start date of the intervention.   

```{r}
plot(impact)
```

We can see that the modelled counterfactual increases in the campaign period, so
too does the actual website sessions. Rather than rely on prior period comparisons
we are able to extract the pointwise and cumulative uplift in a more reasoned way.  

We can also generate a written analysis report.  We can see the estimate
effect size is 9.91 extra sessions per day, very close to the +10 we injected
in our made-up example.  

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
summary(impact, "report")
```

## Comparing to other methods  

If we rely on a simple, more naive method in this case, we can overestimate the 
impact of our TV commercial.  

By comparing the average daily web sessions before vs. after the campaign started
we see an uplift near +25 visits per day. We know this is an over-estimation as 
we only injected an uplift of +10 when we created the synthetic data.  

```{r echo=FALSE, message=FALSE, warning=FALSE}
raw <- as.data.frame(data) %>% 
  mutate(time = 1:100,
         period = ifelse(time < 71, "pre", "post")) %>% 
  group_by(period) %>% 
  mutate(period_ave = mean(y))

ggplot(raw, aes(time, y, group = 1)) +
  geom_line() +
  geom_line(data = filter(raw, period == "pre"), aes(time, period_ave), colour = "red") +
  geom_line(data = filter(raw, period == "post"), aes(time, period_ave), colour = "blue") +
  geom_vline(xintercept = 70, lty = 2) +
  annotate("text", x = 50, y = 125, label = "Uplift using prior period \n comparison is ~ 25 visits per day") +
  labs(title = "How does our measure of uplift change using more basic methods?",
       subtitle = "By comparing average daily visits before and after the tv ad leads to overestimating uplift",
       x = "day",
       y = "Daily Web Visits (simulated data)") +
  theme_bw()
```







